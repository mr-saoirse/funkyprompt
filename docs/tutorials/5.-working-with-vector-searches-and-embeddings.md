---
description: Embeddings, Pydantic types and Vector Spaces
---

# 5. Working with vector searches and embeddings

To understand embeddings, lets insert two pdfs, each with the `open-ai` embedding and the `instruct` embedding. The instruct embedding is not bundled automatically with `funkyprompt` so you need to install it with;

```
pip install InstructorEmbedding
```

There is a utility to load pdf documents which we will use shortly. It looks like this;

```python
from funkyprompt.io.tools.ingestion import ingest_pdf
ingest_pdf(<StoreName>, file, embedding_provider='instruct') #default to open-ai
```

In this section we will compare two embeddings for asking questions. Im going to load two pdf books into two different embedding stores. Lets see how this is setup first.

In `funkyprompt` we lean on function signatures or Pydantic as much as we can to define contracts. There should be just one way to configure things. This goes for configuring what embedding to use - its part of the type. By default the `AbstractVectorStoreEntry` uses the open-ai embedding for simplicity but its certainly not the only choice or the best choice.&#x20;

{% hint style="info" %}
We can use different embeddings with our vector store and we control it by the Pydantic types, which know the vector lengths to use in setting up the `pyarrow` schema that we use in the vector store
{% endhint %}

You can inherit from any base type and add at least one `vector` property. To illustrate we will create a general "Instruct" base type. The default embedding for the `AbstractContentModel` is the `openai` embedding provider.

```python
class InstructEmbeddingContentModel(AbstractContentModel):
    """ """
    vector: Vector(
        EmbeddingFunctions.instruct.ndims()
    ) = EmbeddingFunctions.instruct.VectorField()
    content: str = EmbeddingFunctions.instruct.SourceField()

```

Observe that we configure the embedding provider and also configure the vector length. Now we can create a VectorStore from this or something constructed/inherited from this.&#x20;

{% hint style="info" %}
In the early versions of funkyprompt I wrote all the Pydantic mappings and Embedding management from scratch. But it turns out LanceDB has some great semantics for combining the underlying lance/pyarrow schema, the embeddings and Pydantic. You can read more about it [here](https://lancedb.github.io/lancedb/embeddings/) - above we have use the Lance approach which means we can add and search content and LanceDB will apply the embeddings under the hood making for simpler code!
{% endhint %}

```python
from funkyprompt.io import VectorDataStore
store = VectorStore(InstructEmbeddingContentModel, description='my store')
```

Under the hood, the `ingest_pdf` simply creates a model and store in this way, constructing a model for the right embedding provider and then ingesting data into the vector store as discussed in the previous Tutorial.

***

Now that you understand how to define types and create vector stores (you should play around with creating types and adding data to get a feel for it of course) we can now do some analysis on some data. Lets ingest some data - note that the instruct embedding is considerably slower than the open-ai one.

<pre class="language-python"><code class="lang-python"><strong>book_urls = [] #fill this in with some books you have in pdf/epub/etc.
</strong><strong>for embedding_provider in ['open-ai', 'instruct']:
</strong>    for book in book_urls:
        ingest_pdf(f"BookChapters-{embedding_provider}", 
                   book, 
                   #add a doc_id or other lables if required
                   embedding_provider=embedding_provider)  
</code></pre>

Todays exercise will be the visualize how these to books are embedded and how questions are loading different vectors using different similarity metrics. &#x20;

After we ingest these we can use stores to do vector searches for each type.

{% code overflow="wrap" %}
```python
from funkyprompt.io import VectorDataStore
#you can either create the type / import the type or dynamically infer it by name
open_ai_store = VectorStore._load_vector_store("BookChapters-open-ai")
instruct_store = VectorStore._load_vector_store("BookChapters-instruct", embeddings_provider='instruct') 
```
{% endcode %}

In my case I used two books that are about different things and I want to understand how they are represented and retrieved. My books are Longitude by Sobel, and Hidden Order by John Holland. One interesting question I have is what happens when I asked about both topics. Do I get results for both or only vectors for one?

<pre class="language-python" data-overflow="wrap"><code class="lang-python">from funkyprompt import agent
store = VectorDataStore._load_vector_store("OpenAiBookChapters")
#there are many ways to do this but this example illustrates using store as function
#funkyprompt allows agents to use any functions passed in or discover functions
context = "Provides details about books - if you are not sure about context run this function"
agent("Who invented the clock H-4?", [store.as_function_description(context)])
#answer: The clock H-4 was invented by John Harrison.

<strong>#question: Where is the clock H-4 located today
</strong>#answer: The H-4 clock, often hailed as the most important timekeeper ever built, is currently located in the National Maritime Museum in London. It draws millions of visitors each year. It's not operational anymore to preserve it and prevent wear and tear.
</code></pre>

We can ask each store the following question (I have not shown debug which shows the vector search results but we will be diving into that directly shortly)

{% code overflow="wrap" %}
```
Question: What are John Holland's and Sobel's books about respectively?
<AGENT OVER OPEN AI STORE ANSWER>
John Holland\'s book is titled "Hidden Order: How Adaption Builds Complexity". It seems to delve into the subject of adaptive control systems and the mathematical models behind them. This suggests that the book is about how complex systems evolve and thrive by adapting to their environments.\n\nAs for Sobel, one of their books is about the life of John Harrison, a self-educated man who from humble beginnings rose to prominence due to his inventiveness and diligence. Specifically, the excerpt talks about his journey to solve a major issue of his time â€“ creating a timekeeping device that could withstand the conditions at sea. This tells us that the book follows the historical narrative of Harrison\'s life, highlighting his work on creating reliable maritime navigation tools.\n\nThe analysis provided by the run_search function is about the content of the books, not the books themselves. Additional context about the exact title of Sobel\'s book would help make this response more accurate.
<AGENT OVER INSTRUCT STORE ANSWER>
John Holland\'s book "Hidden Order: How Adaptation Builds Complexity" is about understanding the mechanisms behind the creation of organized systems. It delves into the understanding of natural processes such as evolution, immune systems, growth and development, and learning, that all have salient features of complexity that can be understood in terms of adaptive systems.\n\nAs for Sobel, it seems that there are multiple authors with this surname. The provided excerpts refer to Dava Sobel\'s "Longitude: The True Story of a Lone Genius Who Solved the Greatest Scientific Problem of His Time" which is about the life of John Harrison. He was a carpenter\'s son who invented a clock that could be used to determine longitude at sea, a problem that had baffled scientists and navigators for centuries.\n\nTo provide a more accurate answer about the book of \'Sobel\', it would be helpful if you could provide the full name of the author.
```
{% endcode %}

We are going to to use what I will call a _divergent_ question to study vector search. This is _divergent_ for us because it coverts two "spaces" or two books but we want to match both.

{% code overflow="wrap" %}
```
What can you tell me about emergence of complexity in natural systems through evolution or the invention of the clock H-4 as mentioned in the book by Sobel?
```
{% endcode %}

This question can be asked as one question and we can look at the top 10 vector results under different circumstances OR the LLM can recognize the divergence and ask N separate queries each which might have better luck. Lets take a look...

_Top 10 Vector Results for the question as a whole:_ Running this produces 10 results from Holland's Complexity book with distances of about 0.28 on average

_Top 10 Vector Results for the question about complexity:_ Running this produces 10 results from Holland's Complexity book with distances of about 0.24 on average

_Top 10 Vector Results for the question about the clock:_ Running this produces 10 results from Sobel's Longitude book with distances of about 0.28 on average

{% code overflow="wrap" %}
```
What can you tell me about telling me about Sobels book on Time and Longitude Or Hollands book on the emergence of complexity and hidden order
```
{% endcode %}

This question returns a good interspersed mix of results from both book excerpts with scores close to 0.3

These illustrate the idea. In the last case we would get data on both parts of the question but in some cases one part of the question can "drown" out the results from the other part because the topic is "divergent". We can get around this by asking multiple questions in parallal as mentioned briefly above but lets see if we can understand a little more the shape of the vector space.

### Visualize&#x20;

We will use [UMAP](https://github.com/lmcinnes/umap/tree/master)[ ](https://github.com/lmcinnes/umap/tree/master)to understand vectors. You can install this and any other libraries that you need to follow along in a Jupyter notebook. To start with

```bash
pip install umap-learn[plot]
```

Now we take the three questions - the composite and the separate questions

{% code overflow="wrap" %}
```
q0: 'What can you tell me about emergence of complexity in natural systems through evolution and the invention of the H-4 clock for sailing and navigation',
q1: 'What can you tell me about emergence of complexity in natural systems through evolution',
q2: 'what can you tell me about the invention of the H-4 clock for sailing and navigation'
```
{% endcode %}

<figure><img src="../.gitbook/assets/image (1).png" alt="" width="563"><figcaption><p>Two books and three questions</p></figcaption></figure>

We see that both the composite question and the question about complexity is near the HiddenOrder book while the question about the _H-4 clock_ is near the _Longitude_ book. The "problem" here is that if we ask Q0, we will not retrieve anything about the clock.

With this intuition the vector search function is design to "prompt" the LLM to not query by a single question but to ask multiple questions in parallel. Vector search is relatively fast. We can then prune the best of the (locally) best answers.

{% code overflow="wrap" %}
```python
agent("What can you tell me about emergence of complexity in natural systems through evolution and the invention of the H-4 clock for sailing and navigation", 
      [store.as_function_description()])
```
{% endcode %}

Under the hood it uses the function like this - note that it knows to split up the question by itself which is nice and reduces round trips

{% code overflow="wrap" %}
```json
{
  "role": "assistant",
  "content": null,
  "function_call": {
    "name": "run_search",
    "arguments": "{\n  \"queries\": [\"emergence of complexity in natural systems through evolution\", \"invention of the H-4 clock for sailing and navigation\"],\n  \"limit\": 5\n}"
  }
}
```
{% endcode %}

I provide a sumary of the response for brevity - i generated the summary by repassing it into the LLM

{% code overflow="wrap" %}
```
The text discusses the concept of the emergence of complexity in natural systems, where simple components self-organize to form more intricate structures and behaviors. This is exemplified in various natural systems, including the diversification of species in a rainforest, neuronal networks in the brain, and the specialization of firms in an economy, where agents adapt and fill specific roles or "niches" through interactions, resulting in a diverse and interdependent system. Additionally, it highlights John Harrison's H-4 clock, which revolutionized navigation by accurately calculating longitude and underscores how agents, whether organisms, parts of a system, or inventors, have the capacity to drive complex developments from relatively simple beginnings.
```
{% endcode %}

The important point is that it describes both accounts. However, without the multiple query synthesis I was unable to get any details about Longitude with the single question above.

This suggests only that in general the stores (e.g. the Vector Store) should accept parameters that are plural with a single question as a special case. This is both (a) efficient because the LLM can be instructed to generate plurality of questions in one shot and (b) can avoid one part of a question over shadowing another in the vector searches overall.

_Visual of OpenAI embedding_

Before moving on, also plot the OpenAI embedding. Note there is a stochastic element to these and over several realizations I am making for qualitative observations. Qualitatively the two embeddings separate the books. I get the impression that the OpenAI seem slight more compact or densely packed but I am not sure. Also, there is a datapoint from _Longitude_ that consistently appears in the _HiddenOrder_ section.

<figure><img src="../.gitbook/assets/image (1) (1).png" alt="" width="563"><figcaption><p>Open AI embeddings in store with UMAP visual</p></figcaption></figure>

It turns out when we use the interactive UMAP to see what the text data point is its the single word "solution" - most of these data points represent much larger text paragraphs.&#x20;

{% hint style="info" %}
To use the store to show the UMAP (with optional new questions) for different types of plots such as _interactive_ use the `store.plot(questions=questions, plot_type='interactive')`
{% endhint %}

