{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AbstractModel(BaseModel):\n",
    "    name: str\n",
    "     \n",
    "    @classmethod\n",
    "    @property\n",
    "    def entity_name(self):\n",
    "        return \"test\"\n",
    "    \n",
    "    @classmethod\n",
    "    @property\n",
    "    def entity_namespace(cls):\n",
    "        return \"test\"\n",
    "    \n",
    "    @classmethod\n",
    "    @property\n",
    "    def fullname(cls):\n",
    "        return f'{cls.entity_name}/{cls.entity_namespace}'\n",
    "    \n",
    "    \n",
    "    #create model, from data etc.\n",
    "    \n",
    "\n",
    "class AbstractContentModel(LanceModel, AbstractModel):\n",
    "    \"\"\"\n",
    "    MyModel = AbstractContentModel(name='test', content='test', vector=nd.zeros(EmbeddingFunctions.openai.ndims()))\n",
    "    \n",
    "    \"\"\"\n",
    "    content: str = EmbeddingFunctions.openai.VectorField()\n",
    "    vector: Vector(EmbeddingFunctions.openai.ndims()) = EmbeddingFunctions.openai.SourceField()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import duckdb\n",
    "\n",
    "class DuckDBClient:\n",
    "    def __init__(self, **options):\n",
    "        self._cursor = duckdb.connect()\n",
    "        AWS_ACCESS_KEY_ID = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "        AWS_SECRET_ACCESS_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "        AWS_DEFAULT_REGION = os.environ[\"AWS_DEFAULT_REGION\"]\n",
    "        if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:\n",
    "            creds = f\"\"\"\n",
    "                SET s3_region='{AWS_DEFAULT_REGION}';\n",
    "                SET s3_access_key_id='{AWS_ACCESS_KEY_ID}';\n",
    "                SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';\"\"\"\n",
    "\n",
    "        self._cursor.execute(\n",
    "            f\"\"\"\n",
    "            INSTALL httpfs;\n",
    "            LOAD httpfs;\n",
    "            {creds}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    def inspect_enums(\n",
    "        self, uri, enum_threshold=200, max_str_length=100, omit_fields=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        inspect enums is used to send context to LLM\n",
    "        dont use this if you have sensitive data in fields or add protection\n",
    "\n",
    "        for example this can be used if we ask vague questions\n",
    "        or questions that reference misspelled or alternately spelt data - the LLM can make sense of it\n",
    "\n",
    "        this is probably necessary for SQL types to be useful but avoides sending too much data in context\n",
    "        \"\"\"\n",
    "        df = self.execute(f\"SELECT * FROM '{uri}'\")\n",
    "\n",
    "        def try_unique(c):\n",
    "            try:\n",
    "                # dont allow big strings (polars notation)\n",
    "                l = df[c].str.lengths().mean()\n",
    "                # filter by sending back max threshold in these cases\n",
    "                if l > max_str_length or c in (omit_fields or []):\n",
    "                    return enum_threshold\n",
    "                # if we are happy, return the list of enumerated values for LLM context\n",
    "                return len(df[c].unique())\n",
    "\n",
    "            except:\n",
    "                return enum_threshold\n",
    "\n",
    "        columns = df.columns\n",
    "        enum_types = [c for c in columns if try_unique(c) < enum_threshold]\n",
    "        return {c: list(df[c].unique()) for c in df.columns if c in enum_types}\n",
    "\n",
    "    def execute(self, query):\n",
    "        \"\"\" \"\"\"\n",
    "        return self._cursor.execute(query).pl()\n",
    "\n",
    "    def query_from_root(self, root):\n",
    "        root = root.rstrip(\"/\")\n",
    "        if root[-1 * len(\".parquet\") :] != \".parquet\":\n",
    "            root += \"/*.parquet\"\n",
    "        return _query(self, root)\n",
    "    \n",
    "class StoreSearchVector:\n",
    "    pass\n",
    "\n",
    "class QueryOptions(BaseModel):\n",
    "    limit: int = 5\n",
    "    probes: int: 20\n",
    "    metric: str = 'l2'\n",
    "    refine_Factor: int = 10\n",
    "    columns: typing.List[str] = ['name', 'content']\n",
    "\n",
    "class VectorStoreBase:\n",
    "    \"\"\"\n",
    "    In funkyprompt we have a generic store that implements search semantics over stores\n",
    "    \"\"\"\n",
    "    def __init__(self, model: AbstractContentModel, description:str, store_vector: StoreSearchVector=None):\n",
    "        \"\"\"\n",
    "        **Args**\n",
    "            model: A pydantic model that inheirts from a suitable schema aware base\n",
    "            description: a description of the store that can be registered for search\n",
    "            store_vector: a funcky vectorized description of how and when to use the store\n",
    "        \n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        self._description = description\n",
    "        self._store_vector = store_vector\n",
    "        \n",
    "        self._db_uri = f\"{VECTOR_STORE_ROOT}/{self._model.entity_namespace}\"\n",
    "        self._table_uri = self._db_uri = f\"{self._db_uri}/{self._model.entity_name}.lance\"\n",
    "        self._table = self._open_table()\n",
    "        self._duck_client = DuckDBClient()\n",
    "            \n",
    "    def _get_lance_connection(self):\n",
    "        #from env + do some s3 stuff\n",
    "        #os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "        #os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "        #return lancedb.connect(root, region=os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "\n",
    "        db = lancedb.connect(self._db_uri)\n",
    "        return db\n",
    "\n",
    "    def _open_table(self):\n",
    "        db = self._get_lance_connection()\n",
    "        name = self._model.entity_name\n",
    "        if name in db:\n",
    "            return db[name]\n",
    "        self.register_store()\n",
    "        return db.create_table(name, schema=self._model)\n",
    "    \n",
    "    def register_store(self):\n",
    "        \"\"\"\n",
    "        upsert the description and components that we use to discover this store\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def query_dataset(self, query):\n",
    "        dataset = lance.dataset(self._table_uri)\n",
    "        return self._duck_client.execute(query)\n",
    "\n",
    "    def load(self, limit=None):\n",
    "        \"\"\"\n",
    "        returns the polars data for the records\n",
    "        \"\"\"\n",
    "        dataset = lance.dataset(self._table_uri)\n",
    "        #logger.debug(f\"Fetching from {self._table_uri}\")\n",
    "        limit = f\"LIMIT {limit}\" if limit else \"\"\n",
    "        return self._duck_client.execute(f\"SELECT * FROM dataset {limit}\")\n",
    "    \n",
    "    def add(self, records: typing.List[AbstractContentModel], key_field=None):\n",
    "        \"\"\"\n",
    "        Add record(s) to the store using the correct schema\n",
    "        \n",
    "        **Args**\n",
    "           records: Alist of abstract entities (or dicts taht confirm to that schema)\n",
    "           key_field: The pyantic type either by default or config should define the key field for upsert or it can be passed in here\n",
    "        \"\"\"\n",
    "        if not isinstance(records,list):\n",
    "            records = [records]\n",
    "            \n",
    "    \n",
    "    def run_search(self, \n",
    "                   questions: typing.List[str], \n",
    "                   since_date=None, \n",
    "                   #lance db settings + our own system predicates including probe, limit\n",
    "                   query_options: = QueryOptions(), \n",
    "                   #add schema predicates for IN and = to matches for simple filtering\n",
    "                   **predicates):\n",
    "        \"\"\"\n",
    "        the search exposed to an agent\n",
    "        aribtrary predicates can be passed if the schema of the store is known but usually it will not be known to the agent\n",
    "        the agent should focus on text and possible date related queries\n",
    "        \n",
    "        **Args**\n",
    "           questions: one or more questions to ask the store. Recommend full sentences.\n",
    "           since_date: for restricting vector searches post some date\n",
    "           query_options: underlying query options - usually the defaults are sufficient      \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def plot(self, **options):\n",
    "        pass\n",
    "\n",
    "\n",
    "store = VectorStoreBase(AbstractContentModel, description=\"A store for search for some things\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1783929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148949de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
